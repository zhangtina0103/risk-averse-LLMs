# Requirements for DPO Training with Hyperparameter Optimization
# Based on Copy_of_dpo_training4 (2).ipynb and hyperparameter_tuning/bayesian_opt.py
# Tested and working on Lambda Cloud with CUDA 12.8

# Core ML/DL Frameworks
# Note: PyTorch is pre-installed on Lambda Cloud, but if needed: torch>=2.1.0
numpy>=1.24.0,<2.0  # Must be <2.0 for Lambda Cloud compatibility with system packages
transformers>=4.46.0  # Required for Qwen3 support (tested with 4.57.5)

# DPO Training Libraries
trl>=0.12.0             # DPO Trainer (tested with 0.26.2, compatible with transformers 4.57.5)
peft>=0.7.0             # LoRA/QLoRA
accelerate>=0.25.0      # Distributed training
bitsandbytes>=0.41.0    # 4-bit quantization for QLoRA

# Data Loading
datasets>=2.16.0

# Hyperparameter Optimization
bayesian-optimization>=1.4.0    # For Bayesian optimization
scikit-learn>=1.3.0             # For random search and utilities

# Utilities
tqdm>=4.66.0            # Progress bars
Pillow>=10.0.0          # Required for transformers image processing (tested with latest)

# Required for transformers compatibility (even for PyTorch-only workflows)
tf-keras                # Needed because transformers tries to import TensorFlow components
