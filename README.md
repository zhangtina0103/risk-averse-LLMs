<a name="readme-top"></a>

<!-- PROJECT LOGO -->
<br />
<div align="center">

<h3 align="center">Direct Preference Optimization from scratch in PyTorch</h3>

  <p align="center">
<!--     <a href="https://github.com/ahmed-alllam/Forward-Forward-Algorithm">View Demo</a> -->
<!--     · -->
    <a href="https://github.com/ahmed-alllam/Direct-Preference-Optimization/issues">Report Bug</a>
    ·
    <a href="https://github.com/ahmed-alllam/Direct-Preference-Optimization/issues">Request Feature</a>
  </p>
</div>

<!-- ABOUT THE PROJECT -->

## About The Project

This project is an implementation of Direct Preference Optimization, an alternative to RLHF for aligning Large Language Models (LLMs) to human. The algorithm is described in the research paper [Direct Preference Optimization: Your Language Model is Secretly a Reward Model
](https://arxiv.org/abs/2305.18290).

Direct Preference Optimization (DPO) is a promising and efficient technique for fine-tuning Large Language Models (LLMs) aligned with human preferences. Compared to traditional Reinforcement Learning From Human Feedback (RLHF), DPO eliminates the need for a separate reward model and simplifies the training process, leading to better stability and computational efficiency.

The key insight in Direct Preference Optimization is replacing the complex reward modeling process in RLHF with a simple loss function that directly optimizes for human preferences in closed form. It does this by simply increasing the log probability of the tokens in the human prefered responses, and decreasing the log probability of the tokens in the human disprefered responses, given a preferences dataset, which basically makes the model have an implicit reward function that is directly optimized for human preferences. Through this clever math trick, the process now becomes much simpler and more efficient than RLHF, as it does not require a separate reward model, and it is also more stable, as it does not use other methods like PPO for fine-tuning.

The DPO loss function is defined as follows:

$$
L_\text{DPO}(\pi_{\theta}; \pi_\text{ref}) = -E_{(x, y_w, y_l)\sim D}\left[\log \sigma \left(
\beta \log \frac{\pi_{\theta}(y_w\mid x)}{\pi_\text{ref}(y_w\mid x)} \thinspace
{- \beta \log \frac{\pi_{\theta}(y_l\mid x)}{\pi_\text{ref}(y_l\mid x)}}\right)\right]
$$

where:

- $\pi_{\theta}$ is the language model we want to fine-tune
- $\pi_\text{ref}$ is a reference model, usually a frozen version of the original pre-trained language model
- $D$ is the dataset of preferences
- $x$ is a sample prompt from the dataset $D$
- $y_w$ is the human prefered response to the prompt $x$
- $y_l$ is the human disprefered response to the prompt $x$
- $\beta$ is a hyperparameter that controls the amount of divergence from the reference model $\pi_\text{ref}$

The DPO loss function can be broken down into two main terms, the first term represents the log probability of the human-preferred response $y_w$. This term aims to maximize the probability of $y_w$ as generated by the model $\pi_{\theta}$, relative to the reference model $\pi_{\text{ref}}$. The division by $\pi_{\text{ref}}$ serves as a regularizing factor, ensuring that the fine-tuning does not cause the model to deviate excessively from its original training. Maximizing this term effectively increases the likelihood of $\pi_{\theta}$ generating responses similar to $y_w$ in response to inputs like $x$, reinforcing the human preference patterns. Conversely, the second term focuses on minimizing the log probability of the human-dispreferred response $y_l$. This is achieved by reducing the model's tendency to generate $y_l$ type responses, as indicated by the negative sign.

The hyperparameter $\beta$, typically set between 0.1 and 0.5, affects the amount of divergence from the reference model $\pi_\text{ref}$, allowing for controlled adjustments in the model's outputs while preventing significant deviations from the behavior of the reference model. The entire computation is then simply averaged across the dataset $D$ or a batch of samples from it, giving us the final DPO loss that we can optimize for using gradient descent to fine-tune the language model.

For a detailed explanation, you can check my blog post [Unveiling the Hidden Reward System in Language Models: A Dive into DPO](https://allam.vercel.app/post/dpo/)

[

## Risk-Averse Fine-Tuning (DPO) Quick Start

This fork uses DPO to fine-tune a base LLM toward risk-averse choices. We use a dataset of choice situations where a risk-averse agent and a risk-neutral agent disagree; the risk-averse label becomes the preferred response in DPO.

### 1) Create DPO pairs from JSON

If you already converted the Excel to JSON, generate DPO triples `{prompt, chosen, rejected}`:

```bash
python dpo_data_generate.py \
  --input ../strict_disagreements_10k_with_prompts_and_bad_formats.json \
  --output risk_averse_dpo.json \
  --sample_size 500
```

Notes:

- `chosen` = risk-averse label, `rejected` = risk-neutral label.
- You can increase `--sample_size` if you want more data.

### 2) Train with DPO (risk-averse objective)

```bash
python train_risk_preferences.py \
  --model_name microsoft/phi-2 \
  --dataset risk_averse_dpo.json \
  --epochs 3 \
  --beta 0.5 \
  --batch_size 4 \
  --lr 1e-5 \
  --wandb_project risk-averse-phi2
```

Tips:

- Increase `--beta` (0.3–0.7) to strengthen the preference toward risk-aversion.
- Start with 500 examples (as in small-data DPO results), then scale.

### 3) Inference check

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("./results-risk-averse/final-model")
tokenizer = AutoTokenizer.from_pretrained("./results-risk-averse/final-model")

prompt = "<your choice-situation prompt>"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=120, temperature=0.7)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

You should see the model favor risk-averse options more consistently than the base model.
